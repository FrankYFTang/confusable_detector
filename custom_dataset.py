# Copyright (C) 2020 and later: Google, Inc.

import configparser
import functools
import numpy as np
import os
import pathlib
import tensorflow as tf
import tensorflow_addons as tfa


# AUTOTUNE allows TensorFlow to find a good allocation of CPU budget for
# performance optimization
AUTOTUNE = tf.data.experimental.AUTOTUNE

class DatasetBuilder:
    def __init__(self, config_path='configs/sample_config.ini', one_hot=True):
        """Read and set configuration from config file (.ini file) and create
        tf.Dataset object or input function according to configuration.

        Args:
            config_path: Str, path to config (.ini) file.
            one_hot: Bool, whether or not to return label as one-hot encoding.

        Raises:
            ValueError: if values in config file does not have the correct type.
        """
        # Set one-hot encoding setting
        self.ONE_HOT = one_hot

        # Parse config file
        config = configparser.ConfigParser()
        config.read(config_path)

        # Get (and check) configuration
        # Get dataset info
        self._TRAIN_DATA_DIR = config.get('DATASET', 'TRAIN_DATA_DIR')
        self._TEST_DATA_DIR = config.get('DATASET', 'TEST_DATA_DIR')
        self._LABEL_FILE = config.get('DATASET', 'LABEL_FILE')

        # Get image info
        self._HEIGHT = config.getint('IMAGE', 'HEIGHT')
        self._WIDTH = config.getint('IMAGE', 'WIDTH')
        self._GRAYSCALE_IN = config.getboolean('IMAGE', 'GRAYSCALE_IN')
        self._GRAYSCALE_OUT = config.getboolean('IMAGE', 'GRAYSCALE_OUT')

        # Get traning and testing spec
        self._TRAIN_BATCH_SIZE = config.getint('TRAIN_TEST_SPEC',
                                              'TRAIN_BATCH_SIZE')
        self._TEST_BATCH_SIZE = config.getint('TRAIN_TEST_SPEC',
                                             'TEST_BATCH_SIZE')
        self._SHUFFLE_BUFFER_SIZE = config.getint('TRAIN_TEST_SPEC',
                                                 'SHUFFLE_BUFFER_SIZE')
        self._PREFETCH_BUFFER_SIZE = config.getint('TRAIN_TEST_SPEC',
                                                  'PREFETCH_BUFFER_SIZE')

        # Get data augmentation spec
        self._RANDOM_ROTATE = config.getboolean('DATA_AUG', 'RANDOM_ROTATE')
        self._ROTATE_STDDEV = config.getfloat('DATA_AUG', 'ROTATE_STDDEV')

        self._RANDOM_ZOOM = config.getboolean('DATA_AUG', 'RANDOM_ZOOM')
        self._ZOOM_PERCENT = config.getfloat('DATA_AUG', 'ZOOM_PERCENT')
        self._ZOOM_STDDEV = config.getfloat('DATA_AUG', 'ZOOM_STDDEV')

        self._RESIZE = config.getboolean('DATA_AUG', 'RESIZE')
        self._RESIZE_HEIGHT = config.getint('DATA_AUG', 'RESIZE_HEIGHT')
        self._RESIZE_WIDTH = config.getint('DATA_AUG', 'RESIZE_WIDTH')

        # Label conversion
        self._CLASS_NAMES = [] # Class names in the same order as in label file
        with open(self._LABEL_FILE) as f:
            for line in f:
                code_point = line.split('\n')[0]
                self._CLASS_NAMES.append(code_point)
        self._NUM_CLASSES = len(self._CLASS_NAMES) # Number of classes

    def _get_label(self, file_path):
        """Given a path to label file. Expect label file to be in the format
        generated by 'source/generate_source_file.py'.

        Args:
            file_path: Str, path to label file.

        Returns:
            if self._ONE_HOT:
                label: tf.Tensor, one hot encoding of label.
            else:
                idx: tf.Tensor, label index.
        """
        # Convert path to file name
        file_name = tf.strings.split(file_path, os.path.sep)[-1]
        # Derive label from file name
        class_name = tf.strings.split(file_name, '_')[0]
        idx = tf.reduce_min(tf.where(tf.equal(self._CLASS_NAMES, class_name)))
        if self.ONE_HOT:
            label = tf.one_hot(idx, self._NUM_CLASSES)
            return label
        else:
            return idx

    def _decode_img(self, img):
        """Decode image and convert to tf.Tensor.

        Args:
            img: tf.Tensor, image of type String.

        Returns:
            img: tf.Tensor, image of type float32.
        """
        # Convert compressed string to a 3D uint8 tensor
        img = tf.io.decode_png(img)
        # Convert data type to float between 0 and 1
        img = tf.image.convert_image_dtype(img, tf.float32)
        return img

    def _process_path(self, file_path):
        """Process file path to produce image and label tensor.

        Args:
            file_path: Str, path to image file.

        Returns:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor represented by one-hot encoding.
        """
        # Get label and image Tensor
        label = self._get_label(file_path)
        img = tf.io.read_file(file_path)
        img = self._decode_img(img)
        return img, label

    def _process_img_path(self, file_path):
        """Similar to self._process_img_path. Do not require label path! Uses
        first section of file name as label.

        Args:
            file_path: Str, path to image file.

        Returns:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor represented by file name.
        """
        label = tf.strings.split(file_path, os.path.sep)[-1]
        img = tf.io.read_file(file_path)
        img = self._decode_img(img)
        return img, label

    def _convert_format(self, img, label):
        """Convert image format from RGB to grayscale or grayscale to RGB.

        Args:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.

        Returns:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.
        """
        # Convert between graycale and rgb
        if self._GRAYSCALE_IN and not self._GRAYSCALE_OUT:
            # If input is grayscale and output is rgb
            img = tf.image.grayscale_to_rgb(img)  # use tensorflow function
        elif not self._GRAYSCALE_IN and self._GRAYSCALE_OUT:
            # If input is rgb and output is grayscale
            # img = tf.reduce_mean(img, axis=2)
            img = tf.image.rgb_to_grayscale(img)
        return img,label

    def _augment(self, img, label):
        """Data augmentation. Two augmentation method are carried out:
            1. Random rotate: randomly rotate image by x degree. x follows
                normal distribution with 0 as mean as described in config file.
            2. Random zoom: crop (zoom in) on the image by x percent. x follows
                normal distribution with 0 as mean as described in config file.

        Args:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.

        Returns:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.
        """
        # Randomly rotate by -3 and 3 degrees
        if self._RANDOM_ROTATE:
            # Follows normal distribution whose magnitude is more than 2 standard
            # deviations from the mean are dropped and re-picked
            degree = tf.random.truncated_normal(shape=[],
                                                stddev=self._ROTATE_STDDEV)
            # Rounds half to even. Also known as bankers rounding.
            degree = tf.math.round(degree)
            img = tfa.image.rotate(img, degree)

        # Randomly zoom in on image
        if self._RANDOM_ZOOM:
            # Generate 5 crop settings, ranging from a 0% to n% crop.
            scales = list(np.arange((100-self._ZOOM_PERCENT)/100, 1.0, 0.01))
            scales.reverse()
            boxes = np.zeros((len(scales), 4))
            for i, scale in enumerate(scales):
                x1 = y1 = 0.5 - (0.5 * scale)
                x2 = y2 = 0.5 + (0.5 * scale)
                boxes[i] = [x1, y1, x2, y2]
            crops = tf.image.crop_and_resize([img], boxes=boxes,
                                             box_indices=np.zeros(len(scales)),
                                             crop_size=(self._HEIGHT,
                                                        self._WIDTH))
            # I am personally shamed of this implementation here
            # TODO: Change distribution here
            # TODO: Add fault proof
            idx = tf.random.truncated_normal(shape=[], stddev=self._ZOOM_STDDEV)
            idx = tf.math.abs(idx) # idx >= 0
            idx = tf.math.round(idx) # Bankers rounding again
            idx = tf.cast(idx, tf.dtypes.int32)
            img = crops[idx]
        return img, label

    def _resize(self, img, label):
        """Resize image for compatibility with Keras model.
        TODO: Add custom CNN models to avoid resizing

        Args:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.

        Returns:
            img: tf.Tensor: image tensor in type tf.float32.
            label: tf.Tensor: label tensor.
        """
        if self._RESIZE:
            img = tf.image.resize(img, (self._RESIZE_HEIGHT,
                                        self._RESIZE_WIDTH))
        return img, label

    def get_train_dataset(self, filter_size=None):
        """Get training dataset. For the purpose of triplet selection, restrict
        dataset to have certain number of labels if specified. See
        https://www.tensorflow.org/addons/tutorials/losses_triplet.

        Args:
            filter_size: Int or None, if filter_size is None, do nothing.
                         If filter_size is Int, restrict datset to only have
                         filter_size number of classes. The classes to include
                         is randomly selected form all classes.

        Returns:
            ds: tf.Dataset, TensorFlow dataset object for training. Each entry
                is (image, label) pair.
        """
        # Get filename dataset (each entry is a filename)
        data_dir = pathlib.Path(self._TRAIN_DATA_DIR)
        list_ds = tf.data.Dataset.list_files(str(data_dir / '*'))

        # Get labeled dataset (each entry is (image, label) tuple)
        ds = list_ds.map(self._process_path, num_parallel_calls=AUTOTUNE)

        if filter_size:
            # Filter using filter_size
            labels = tf.constant(np.random.choice(1000, filter_size,
                                                  replace=False))
            ds = ds.filter(lambda img, label:
                           tf.reduce_any(tf.equal(label,labels)))

        # Format conversion
        ds = ds.map(self._convert_format)
        # Data augmentation
        ds = ds.map(self._augment, num_parallel_calls=AUTOTUNE)
        # Resizing
        ds = ds.map(self._resize, num_parallel_calls=AUTOTUNE)


        # Shuffle, batch, repeat, prefetch
        ds = ds.shuffle(buffer_size=self._SHUFFLE_BUFFER_SIZE)
        ds = ds.batch(self._TRAIN_BATCH_SIZE)
        ds = ds.prefetch(buffer_size=self._PREFETCH_BUFFER_SIZE)

        return ds

    def get_test_dataset(self):
        """Get test dataset.

        Returns:
            ds: tf.Dataset, TensorFlow dataset object for testing. Each entry
                is (image, label) pair.
        """
        # Get filenames
        data_dir = pathlib.Path(self._TEST_DATA_DIR)
        list_ds = tf.data.Dataset.list_files(str(data_dir / '*'))

        # Get labeled dataset
        ds = list_ds.map(self._process_path, num_parallel_calls=AUTOTUNE)
        # Format conversion
        ds = ds.map(self._convert_format)
        # Resizing
        ds = ds.map(self._resize, num_parallel_calls=AUTOTUNE)

        # Batch, prefetch
        ds = ds.batch(self._TEST_BATCH_SIZE)
        ds = ds.prefetch(buffer_size=self._PREFETCH_BUFFER_SIZE)

        return ds

    def get_filename_dataset(self, data_dir):
        """For prediciton only! No label file needed! Given a directory of
        images, return datatset with images and filenames.

        Args:
            data_dir: Str, path to image directory.

        Returns:
            ds: tf.Dataset, TensorFlow dataset object for training. Each entry
                is (image, filename) pair.
        """
        # Get filenames
        data_dir = pathlib.Path(data_dir)
        list_ds = tf.data.Dataset.list_files(str(data_dir / '*'))

        # Get FAKE labeled dataset
        ds = list_ds.map(self._process_img_path, num_parallel_calls=AUTOTUNE)
        # Format conversion
        ds = ds.map(self._convert_format)
        # Resizing
        ds = ds.map(self._resize)

        # Batch, prefetch
        ds = ds.batch(1)

        return ds


    def get_train_input_fn(self, input_name):
        """For tf.estimator training. Create train_input_fn that returns a
        tf.Dataset when called. Each entry in tf.Dataset is a
        {input_name: image}, label pair.

        Args:
            input_name: Str, name of the input tensor. Required by tf.estimator.

        Returns:
            train_input_fn: Function, returns tf.Dataset when called.
        """
        def train_input_fn():
            # Get filenames
            data_dir = pathlib.Path(self._TRAIN_DATA_DIR)
            list_ds = tf.data.Dataset.list_files(str(data_dir / '*'))

            # Get labeled dataset
            ds = list_ds.map(self._process_path, num_parallel_calls=AUTOTUNE)
            # Format conversion
            ds = ds.map(self._convert_format)
            # Data augmentation
            ds = ds.map(self._augment, num_parallel_calls=AUTOTUNE)
            # Resizing
            ds = ds.map(self._resize, num_parallel_calls=AUTOTUNE)
            # Prepare for tf.estimator
            ds = ds.map(lambda img, label: ({input_name: img}, label))

            # Shuffle, batch, repeat, prefetch
            ds = ds.shuffle(buffer_size=self._SHUFFLE_BUFFER_SIZE)
            ds = ds.batch(self._TRAIN_BATCH_SIZE)
            ds = ds.repeat()
            ds = ds.prefetch(buffer_size=self._PREFETCH_BUFFER_SIZE)

            return ds
        return train_input_fn

    def get_eval_input_fn(self, input_name):
        """For tf.estimator evaluation. Create eval_input_fn that returns a
        tf.Dataset when called. Each entry in tf.Dataset is a
        {input_name: image}, label pair.

        Args:
            input_name: Str, name of the input tensor. Required by tf.estimator.

        Returns:
            train_input_fn: Function, returns tf.Dataset when called.
        """
        def eval_input_fn():
            # Get filenames
            data_dir = pathlib.Path(self._TEST_DATA_DIR)
            list_ds = tf.data.Dataset.list_files(str(data_dir / '*'))

            # Get labeled dataset
            ds = list_ds.map(self._process_path, num_parallel_calls=AUTOTUNE)
            # Format conversion
            ds = ds.map(self._convert_format)
            # Resizing
            ds = ds.map(self._resize, num_parallel_calls=AUTOTUNE)
            # Prepare for tf.estimator
            ds = ds.map(lambda img, label: ({input_name: img}, label))

            # Batch, prefetch
            ds = ds.batch(self._TEST_BATCH_SIZE)
            ds = ds.prefetch(buffer_size=self._PREFETCH_BUFFER_SIZE)

            return ds
        return eval_input_fn



if __name__ == "__main__":
    db = DatasetBuilder()
    train_input_fn = db.get_train_input_fn(input_name = 'resnet50_input')
    train_ds = train_input_fn()
    for features_batch, labels_batch in train_ds.take(1):
        print(features_batch)
        print(labels_batch)
        # import pdb;pdb.set_trace()
